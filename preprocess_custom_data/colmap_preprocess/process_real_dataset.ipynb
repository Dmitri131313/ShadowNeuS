{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c1b829b-b312-4807-b0bb-543247c1c8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyransac3d in /home/disk2/lingjingwang/miniconda3/envs/shadowneus/lib/python3.9/site-packages (0.6.0)\n",
      "Requirement already satisfied: imageio in /home/disk2/lingjingwang/miniconda3/envs/shadowneus/lib/python3.9/site-packages (2.31.1)\n",
      "Requirement already satisfied: scikit-image in /home/disk2/lingjingwang/miniconda3/envs/shadowneus/lib/python3.9/site-packages (0.21.0)\n",
      "Requirement already satisfied: numpy in /home/disk2/lingjingwang/miniconda3/envs/shadowneus/lib/python3.9/site-packages (from imageio) (1.22.3)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /home/disk2/lingjingwang/miniconda3/envs/shadowneus/lib/python3.9/site-packages (from imageio) (10.0.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /home/disk2/lingjingwang/miniconda3/envs/shadowneus/lib/python3.9/site-packages (from scikit-image) (2023.7.18)\n",
      "Requirement already satisfied: scipy>=1.8 in /home/disk2/lingjingwang/miniconda3/envs/shadowneus/lib/python3.9/site-packages (from scikit-image) (1.11.1)\n",
      "Requirement already satisfied: packaging>=21 in /home/disk2/lingjingwang/miniconda3/envs/shadowneus/lib/python3.9/site-packages (from scikit-image) (23.1)\n",
      "Requirement already satisfied: networkx>=2.8 in /home/disk2/lingjingwang/miniconda3/envs/shadowneus/lib/python3.9/site-packages (from scikit-image) (3.1)\n",
      "Requirement already satisfied: lazy_loader>=0.2 in /home/disk2/lingjingwang/miniconda3/envs/shadowneus/lib/python3.9/site-packages (from scikit-image) (0.3)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /home/disk2/lingjingwang/miniconda3/envs/shadowneus/lib/python3.9/site-packages (from scikit-image) (1.4.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyransac3d imageio scikit-image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084a0e04-645f-4405-a85f-d3bde5322e88",
   "metadata": {},
   "source": [
    "How we capture the data:\n",
    "\n",
    "1. Prepare two cellphones, named `cam` and `light`, both equipped with OpenCamera for focus and exposure locking.\n",
    "\n",
    "2. Mount the `cam` cellphone onto a tripod to capture the ground and object from a stable view.\n",
    "\n",
    "3. Hold the `light` cellphone in your hands.\n",
    "\n",
    "4. Turn off the room lights and enable the flashlight on cellphone `light`.\n",
    "\n",
    "5. Direct the flashlight towards the object to ensure proper illumination, then lock the focus and exposure on both cellphones.\n",
    "\n",
    "6. Begin capturing images. For each shot, move cellphone `light` to a new position while simultaneously capturing one image from `cam` and one from `light`. Repeat this process multiple times.\n",
    "\n",
    "7. For the final capture, position the `light` cellphone near the `cam` to align the flashlight and camera view, then capture the last shot.\n",
    "\n",
    "8. Save the images taken by `cam` into the `camera` directory and the images from `light` into the `light` directory.\n",
    "\n",
    "9. Turn off the flashlight and restore the room lights.\n",
    "\n",
    "10. Replace the object with a checkerboard on the ground while keeping the `cam` tripod fixed.\n",
    "\n",
    "11. Capture the checkerboard from various angles using the `light` cellphone. Save these images into the `light_calibrate` directory.\n",
    "\n",
    "12. Keep the `cam` fixed and capture the first shot of the checkerboard.\n",
    "\n",
    "13. Move the `cam` to capture the checkerboard from multiple views. Save these images, along with the image from the previous step, into the `camera_calibrate` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689194d7-35bf-496a-aa71-30f3a72b5259",
   "metadata": {},
   "source": [
    "You can download the raw data from https://drive.google.com/file/d/1BTycVSIdzeRY0JmyDeeSxksH7dCXLf47/view?usp=sharing and unzip it to `../raw_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aac9922c-37fa-4683-83da-97fa39b9b929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from glob import glob\n",
    "import trimesh\n",
    "import pyransac3d as pyrsc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7668dd-65c6-40cb-874c-77c70824e60f",
   "metadata": {},
   "source": [
    "`input_path` contains the `camera_calibrate` and `light calibrate` which calibrate the intrinsic matrix of camera views and light views, respectively. We perform intrinsic calibration and pass it to colmap to improve accuracy. \n",
    "\n",
    "To capture the calibration images, put a checkerboard on the ground and capture the checkerboard at different angles. Remember the capture the first `camera_calibrate` image at the **same view** when capturing the scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c17aafb-f540-4194-84d8-4e9511862a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = '../raw_data/pink_bear/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c79eac59-0e63-4aad-8f25-c301257b667a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mtx_dist(file_path, nrows, ncols, chess_width=1.0):\n",
    "    \"\"\"\n",
    "    assume nrows >= ncols\n",
    "    \"\"\"\n",
    "    img_paths = sorted(glob(f'{file_path}/*.jpg'))\n",
    "    imgpoints = []\n",
    "    objpoints = []\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
    "    \n",
    "    objp = np.zeros((nrows*ncols,3), np.float32)\n",
    "    objp[:,:2] = np.mgrid[0:nrows,0:ncols].T.reshape(-1,2)\n",
    "    objp[:, 1] = -objp[:, 1]\n",
    "    objp *= chess_width # mm\n",
    "    gray_img = None\n",
    "    for img_path in img_paths:\n",
    "        img = cv2.imread(img_path)\n",
    "        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        found, corners = cv2.findChessboardCorners(gray_img, (nrows, ncols), None)\n",
    "        if not found:\n",
    "            print(f'Warning, cannot detect {img_path}')\n",
    "            continue\n",
    "        corners = cv2.cornerSubPix(gray_img, corners, (11,11), (-1,-1), criteria)\n",
    "        imgpoints.append(corners)\n",
    "        objpoints.append(objp)\n",
    "    ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, gray_img.shape[::-1], None, None)\n",
    "    return mtx, dist, gray_img.shape[::-1], rvecs, tvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef300665-b573-4149-b73c-19edbf000cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is borrowed from IDR: https://github.com/lioryariv/idr\n",
    "def load_K_Rt_from_P(filename, P=None):\n",
    "    if P is None:\n",
    "        lines = open(filename).read().splitlines()\n",
    "        if len(lines) == 4:\n",
    "            lines = lines[1:]\n",
    "        lines = [[x[0], x[1], x[2], x[3]] for x in (x.split(\" \") for x in lines)]\n",
    "        P = np.asarray(lines).astype(np.float32).squeeze()\n",
    "\n",
    "    out = cv2.decomposeProjectionMatrix(P)\n",
    "    K = out[0]\n",
    "    R = out[1]\n",
    "    t = out[2]\n",
    "\n",
    "    K = K / K[2, 2]\n",
    "    intrinsics = np.eye(4)\n",
    "    intrinsics[:3, :3] = K\n",
    "\n",
    "    pose = np.eye(4, dtype=np.float32)\n",
    "    pose[:3, :3] = R.transpose()\n",
    "    pose[:3, 3] = (t[:3] / t[3])[:, 0]\n",
    "\n",
    "    return intrinsics, pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30b6c015-f243-46d0-90e4-42694b36ac74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_colmap_intr(file_path, mtx):\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(f'{mtx[0, 0]:.4f}, {mtx[1, 1]:.4f}, {mtx[0, 2]:.4f}, {mtx[1, 2]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f180e95b-5b54-4690-9e15-2a291e6da8f6",
   "metadata": {},
   "source": [
    "Calibrate the light intrinsic matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6a4d762-abd6-40fd-a4c7-9841222a3d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "mtx, dist, (w, h), _, _ = get_mtx_dist(f'{input_path}/light_calibrate/', 9, 6, 24.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c83f3cd-5bd7-493b-b668-d21b07f2de39",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(f'{input_path}/light_intr.txt', mtx)\n",
    "save_colmap_intr(f'{input_path}/light_intr_colmap.txt', mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "083e7a57-2590-457f-892d-899c99e07b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def undistort_folder(src_file_path, tgt_file_path, mtx, dist):\n",
    "    os.makedirs(tgt_file_path, exist_ok=True)\n",
    "    for file_name in sorted(os.listdir(src_file_path)):\n",
    "        img = cv2.imread(f'{src_file_path}/{file_name}')\n",
    "        dst = cv2.undistort(img, mtx, dist)\n",
    "        cv2.imwrite(f'{tgt_file_path}/{file_name}', dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71ed9fe-e85c-436e-8f44-bc3dd66b8f55",
   "metadata": {},
   "source": [
    "Undistort images using the intrinsics matrix so that we can use the simple perspective camera model. Undistorted images stored in `light_undistort`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2e0a277-a44b-486b-a95c-b25695b60aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for obj in ['pink_bear', 'hamster', 'fries']:\n",
    "    this_input_path = input_path.replace('pink_bear', obj)\n",
    "    undistort_folder(f'{this_input_path}/light', f'{this_input_path}/light_undistort', mtx, dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a4578e-2ed3-4276-82ca-5593df3907ff",
   "metadata": {},
   "source": [
    "The checkerboard has 9x6 patterns, with size 24.25mm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5753feb-e923-4577-8a3f-37013083a879",
   "metadata": {},
   "outputs": [],
   "source": [
    "mtx, dist, (w, h), rvecs, tvecs = get_mtx_dist(f'{input_path}/camera_calibrate/', 9, 6, 24.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e784e77b-8d2e-409f-88e6-abfb97b91550",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmat = cv2.Rodrigues(rvecs[0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f556b3d-9d00-4ce6-bdc0-9c152cba17b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(f'{input_path}/cam_intr.txt', mtx)\n",
    "save_colmap_intr(f'{input_path}/cam_intr_colmap.txt', mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6af75f7-71c5-428a-af08-d54579fc11fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for obj in ['pink_bear', 'hamster', 'fries']:\n",
    "    this_input_path = input_path.replace('pink_bear', obj)\n",
    "    undistort_folder(f'{this_input_path}/camera', f'{this_input_path}/camera_undistort', mtx, dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c817c9-2424-4c72-9ded-c52ef7b3dc83",
   "metadata": {},
   "source": [
    "Prepare the colmap input data. Put the last image of `camera_undistort` to `images` because it is a collocated captured camera view. We use it to obtain the camera view pose with respect to other light views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab3170b2-bfcd-414f-8c91-69eef3720dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for obj in ['pink_bear', 'hamster', 'fries']:\n",
    "    this_input_path = input_path.replace('pink_bear', obj)\n",
    "    os.makedirs(f'{this_input_path}/colmap_workspace/images', exist_ok=True)\n",
    "    cur_idx = 0\n",
    "    for filename in list(sorted(os.listdir(f'{this_input_path}/camera_undistort')))[-1:]:\n",
    "        shutil.copy(f'{this_input_path}/camera_undistort/{filename}', f'{this_input_path}/colmap_workspace/images/{cur_idx:03d}.jpg')\n",
    "        cur_idx += 1\n",
    "    for filename in sorted(os.listdir(f'{this_input_path}/light_undistort')):\n",
    "        shutil.copy(f'{this_input_path}/light_undistort/{filename}', f'{this_input_path}/colmap_workspace/images/{cur_idx:03d}.jpg')\n",
    "        cur_idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18705f5f-8275-40e0-800e-72a5496b3e13",
   "metadata": {},
   "source": [
    "Now we should run COLMAP GUI to obtain camera poses and a sparse pointcloud. Remeber to use the intrinsic data in `cam_intr_colmap` and `light_intr_colmap` for `cam` and `light` respectively. The working directory of COLMAP is `colmap_workspace`, where we store the COLMAP results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5220189-c1ad-4db8-9dbd-f1ff4f83ef21",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = '../raw_data/pink_bear' # change it to 'hamster' and 'fries' and run all cells below to process all the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447a55ac-f7b5-4bfe-ade0-66795e85c6b5",
   "metadata": {},
   "source": [
    "These scripts are adapted from `NeuS`, with slight modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "201888c8-fbb5-4c68-9b15-d7925ccdac63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../raw_data/pink_bear/colmap_workspace/sparse/0 ['cameras.bin', 'images.bin', 'points3D.bin', 'project.ini']\n",
      "Don't need to run COLMAP\n",
      "Post-colmap\n",
      "Images # 44\n",
      "Points (31638, 3) Visibility (31638, 44)\n",
      "Done with imgs2poses\n"
     ]
    }
   ],
   "source": [
    "!python imgs2poses.py {input_path}/colmap_workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90efa6c6-849e-47c6-8395-e7676a52c946",
   "metadata": {},
   "source": [
    "Follow the instructions in https://github.com/Totoro97/NeuS/tree/main/preprocess_custom_data to crop the point cloud of the object, save it as `${input_path}/colmap_workspace/sparse_points_interest.ply`. Additionally, crop the point cloud of the ground and save it as `${input_path}/colmap_workspace/sparse_points_floor.ply`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44f6c545-281a-49f6-8f1c-d31d18359ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process done!\n"
     ]
    }
   ],
   "source": [
    "!python gen_cameras.py {input_path}/colmap_workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d0c0df7-bb79-4433-9030-8fdd4416c109",
   "metadata": {},
   "outputs": [],
   "source": [
    "cameras = np.load(f'{input_path}/colmap_workspace/preprocessed/cameras_sphere.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "591ef2b0-c3af-4fac-a901-23cc237298f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = len(os.listdir(f'{input_path}/colmap_workspace/images/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05d166d3-df69-4fc0-a680-af7d9e31c8e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "965168fd-3c07-49b8-9251-4240304d7ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intr_pose(cameras, i):\n",
    "    scale_mat = cameras[f'scale_mat_{i}'].astype(np.float32)\n",
    "    world_mat = cameras[f'world_mat_{i}'].astype(np.float32)\n",
    "    P = world_mat @ scale_mat\n",
    "    P = P[:3, :4]\n",
    "    intr, pose = load_K_Rt_from_P(None, P)\n",
    "    return intr, pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "543f100d-23ce-43f8-9d7d-a30c42800687",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_scale_mat = cameras['scale_mat_0']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2972e61-b19f-4a95-bec8-715f5b629a48",
   "metadata": {},
   "source": [
    "Obtain the ground pose and normalize the coordinate space. One way is to use the first image in the `camera_calibrate` directory to obtain the ground pose relative to the `cam`. Another way is to estimate a ground plane from the sparse COLMAP pointcloud, which is shown here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abae1a99-632d-4464-b7f7-e34e37d7ab88",
   "metadata": {},
   "source": [
    "`hamster` and `fries` share the same capture environment as `pink_bear`, therefore the calibration is only done once and relevant data is stored in the `pink_bear` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e022dc02-de38-436e-af7d-55d9f4625df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcl = trimesh.load(f'{input_path}/colmap_workspace/sparse_points_floor.ply')\n",
    "cam_scale_mat_inv = cameras['scale_mat_inv_0']\n",
    "pcl.vertices[:] = pcl.vertices[:] @ cam_scale_mat_inv[:3, :3].T + cam_scale_mat_inv[:3, 3]\n",
    "pcl.vertices[:] = pcl.vertices[:] @ rmat\n",
    "plane = pyrsc.Plane()\n",
    "best_eq, _ = plane.fit(pcl.vertices, 0.01)\n",
    "pcl.export(f'{input_path}/colmap_workspace/sparse_points_normalized.ply');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb03dbee-753f-431d-8b07-329476006fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "floor_height = -float(best_eq[-1])/float(best_eq[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ae80652-3bfc-4455-9568-a767bf6700e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.027370467744678672"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "floor_height"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b567c2-ed44-4838-bedf-9477891052c3",
   "metadata": {},
   "source": [
    "Visualize the light poses at a point cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "009bfda2-a8e1-4bdb-9f43-1459bc98db16",
   "metadata": {},
   "outputs": [],
   "source": [
    "intrinsics = []\n",
    "poses = []\n",
    "points = []\n",
    "for i in range(n_images):\n",
    "    intr, pose = get_intr_pose(cameras, i)\n",
    "    pose[:3] = rmat.T @ pose[:3]\n",
    "    points.append(pose[:3, 3])\n",
    "    intrinsics.append(intr)\n",
    "    poses.append(pose)\n",
    "points = np.stack(points)\n",
    "trimesh.PointCloud(vertices=points).export(f'{input_path}/colmap_workspace/cam_lights.ply');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f896d021-7a75-4be1-bba1-2be7f177cfae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44, 44)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(intrinsics), len(poses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73188e1-230b-4acc-bb27-434011f2e420",
   "metadata": {},
   "source": [
    "Draw a mask and save it to `mask/mask.png`. The mask is conservative esimate of which pixels are definitely the ground."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa21f56-1c26-4393-960d-0487dc94ccc5",
   "metadata": {},
   "source": [
    "Finally, output training data to `training_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "50ec4510-238d-400f-a046-ee3d08a2f544",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, w, h = 347, 189, 3200, 1800 # crop parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "404063fe-8401-408b-abff-ede6a7e3c852",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = f'{input_path}/training_data'\n",
    "os.makedirs(f'{output_path}/image', exist_ok=True)\n",
    "res = {}\n",
    "cam_intr = intrinsics[0].copy()\n",
    "cam_intr[0, 2] -= x\n",
    "cam_intr[1, 2] -= y\n",
    "res.update({\n",
    "    'camera_intrinsics': cam_intr,\n",
    "    'camera_pose': poses[0],\n",
    "    'light_pose': np.stack(poses[1:]),\n",
    "    'floor_height': floor_height,\n",
    "})\n",
    "for i, img_name in enumerate(sorted(os.listdir(f'{input_path}/camera'))):\n",
    "    img = cv2.imread(f'{input_path}/camera/{img_name}')\n",
    "    img = img[y: y+h, x: x+w]\n",
    "    cv2.imwrite(f'{output_path}/image/{i:03d}.jpg', img)\n",
    "mask = cv2.imread(f'{input_path}/mask/mask.png')\n",
    "mask = mask[y: y+h, x: x+w]\n",
    "cv2.imwrite(f'{output_path}/mask.png', mask)\n",
    "np.savez(f'{output_path}/params.npz', **res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c37024e4-055c-46b5-87d6-cdcaa5999f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = np.load(f'{input_path}/training_data/params.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4064f701-38f7-4626-81c8-753c7a8539f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['camera_intrinsics', 'camera_pose', 'light_pose', 'floor_height']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(params.keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
